### to start:
```bash
docker compose up
```
### access web ui:
http://127.0.0.1:8000/

### how it works:

PDF Upload & Processing

1. PDF Upload (FastAPI /upload endpoint)
   1. File validation and UUID generation
   2. Save file and store metadata in MongoDB
   3. Queue background processing task

2. PDF Processing Pipeline
   1. Extract text, images, and tables using PyMuPDF, Camelot, PDFPlumber
   2. Generate markdown with image placeholders
   3. Run OCR on images using PaddleOCR
   4. Clean and normalize markdown content
   5. Process diagrams using LLaVA vision model
   6. Split content into chunks and create vector documents
   7. Generate document summary using Ollama LLM
   8. Store documents in ChromaDB vector store

Vector Storage & Indexing

3. Vector Store (ChromaDB)
   1. Persistent storage with document embeddings
   2. Metadata tracking (file_id, chunk_index, source_file)
   3. Embeddings generated by nomic-embed-text model

4. RAG Pipeline (LlamaIndex)
   1. Create index from ChromaDB documents
   2. Configure with Ollama LLM and embeddings
   3. Set up query engine for similarity search

Question Answering

5. Question Processing (FastAPI /ask endpoint)
   1. Receive question and query vector store
   2. Retrieve top-k similar chunks
   3. Generate answer using Ollama LLM with retrieved context
   4. Return structured response with sources and confidence score

Supporting Services

6. Core Services
   1. Ollama Service: LLM (llama3.2:3b), embeddings (nomic-embed-text), vision (llava:7b)
   2. MongoDB: File metadata and status tracking
   3. OCR Service: PaddleOCR for image text extraction
   4. Markdown Cleaner: Content normalization

**Data Flow:**
PDF -> Text/Image Extraction -> Markdown -> OCR -> Cleaning -> Chunking -> Vector Store -> Question -> Similarity Search -> LLM Response